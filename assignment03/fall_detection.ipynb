{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4ca67e",
   "metadata": {},
   "source": [
    "# Fall Detection using IMU Sensor Data\n",
    "This notebook builds a classifier to distinguish falls from non-falls using data from triaxial accelerometer, gyroscope, and magnetometer sensors. It also compares feature importance across models using SHAP and Plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d3439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/marcoreis/Insync/marco.a.reis@gmail.com/Google Drive/dutfpr/recognition/assignments/recog-assig02/assignment03\n",
      "Files in ../data/falls:\n",
      "[]\n",
      "Files in ../data/falls: []\n",
      "Absolute falls folder path: /Users/marcoreis/Insync/marco.a.reis@gmail.com/Google Drive/dutfpr/recognition/assignments/recog-assig02/data/falls\n",
      "Files in falls folder: []\n",
      "All files in falls folder: []\n",
      "Absolute falls folder path: /Users/marcoreis/Insync/marco.a.reis@gmail.com/Google Drive/dutfpr/recognition/assignments/recog-assig02/data/falls\n",
      "Files in falls folder: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import shap\n",
    "import plotly.express as px\n",
    "import xgboost as xgb\n",
    "\n",
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "for label_folder in ['falls', 'nonfalls']:\n",
    "    folder_path = os.path.join(\"../data\", label_folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"Files in {folder_path}:\")\n",
    "        print(os.listdir(folder_path))\n",
    "        \n",
    "# DATA_FOLDER = \"../data\"  # Change to your folder name or absolute path\n",
    "# # DATA_TRAIN_FOLDER = \"../data/train\"  # Change to your folder name or absolute path\n",
    "# # DATA_TEST_FOLDER = \"../data/test\"  # Change to your folder name or absolute path\n",
    "# SUBMISSION_FOLDER = \"../submission\"  # Change to your desired folder\n",
    "\n",
    "# # Make sure the folder exists\n",
    "# os.makedirs(DATA_FOLDER, exist_ok=True)\n",
    "# os.makedirs(SUBMISSION_FOLDER, exist_ok=True)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Files in ../data/falls:\", os.listdir(os.path.join(\"../data\", \"falls\")))\n",
    "\n",
    "falls_folder = os.path.abspath(os.path.join(\"../data\", \"falls\"))\n",
    "print(\"Absolute falls folder path:\", falls_folder)\n",
    "print(\"Files in falls folder:\", os.listdir(falls_folder))\n",
    "\n",
    "print(\"All files in falls folder:\", os.listdir(falls_folder))\n",
    "\n",
    "# ls -l \"../data/falls\"\n",
    "\n",
    "falls_folder = os.path.abspath(os.path.join(\"../data\", \"falls\"))\n",
    "print(\"Absolute falls folder path:\", falls_folder)\n",
    "print(\"Files in falls folder:\", os.listdir(falls_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39de642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in falls folder: []\n"
     ]
    }
   ],
   "source": [
    "falls_folder = os.path.join(\"../data\", \"falls\")\n",
    "print(\"All files in falls folder:\", os.listdir(falls_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "969efeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falls train shape: (0,) (0,)\n",
      "Falls test shape: (0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "def extract_features_from_file(filepath):\n",
    "    mat = scipy.io.loadmat(filepath)\n",
    "    data = mat['newData']\n",
    "    imu_data = data[:, 1:10]\n",
    "    means = np.mean(imu_data, axis=0)\n",
    "    max_vals = np.max(imu_data, axis=0)\n",
    "    std_devs = np.std(imu_data, axis=0)\n",
    "    return np.concatenate([means, max_vals, std_devs])\n",
    "\n",
    "def split_falls_by_suffix(folder, train_range, test_range):\n",
    "    train_files, test_files = [], []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.mat'):\n",
    "            if any(file.endswith(f\"{i:02d}.mat\") for i in train_range):\n",
    "                train_files.append(os.path.join(folder, file))\n",
    "            if any(file.endswith(f\"{i:02d}.mat\") for i in test_range):\n",
    "                test_files.append(os.path.join(folder, file))\n",
    "    return train_files, test_files\n",
    "\n",
    "falls_folder = os.path.join(\"../data\", \"falls\")\n",
    "train_range = range(1, 19)   # 01 to 18\n",
    "test_range = range(19, 23)   # 19 to 22\n",
    "\n",
    "train_files, test_files = split_falls_by_suffix(falls_folder, train_range, test_range)\n",
    "\n",
    "# Now extract features for train and test\n",
    "X_train_falls = np.array([extract_features_from_file(f) for f in train_files])\n",
    "y_train_falls = np.full(len(train_files), 2)  # Label 2 for falls\n",
    "\n",
    "X_test_falls = np.array([extract_features_from_file(f) for f in test_files])\n",
    "y_test_falls = np.full(len(test_files), 2)    # Label 2 for falls\n",
    "\n",
    "print(\"Falls train shape:\", X_train_falls.shape, y_train_falls.shape)\n",
    "print(\"Falls test shape:\", X_test_falls.shape, y_test_falls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12df4f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in falls folder: []\n"
     ]
    }
   ],
   "source": [
    "falls_folder = os.path.join(\"../data\", \"falls\")\n",
    "print(\"All files in falls folder:\", os.listdir(falls_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acb441b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: []\n",
      "Test files: []\n"
     ]
    }
   ],
   "source": [
    "def split_falls_by_suffix(folder, train_range, test_range):\n",
    "    train_files, test_files = [], []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith('.mat'):\n",
    "            # Train: suffix 01 to 18\n",
    "            if any(file.endswith(f\"{i:02d}.mat\") for i in train_range):\n",
    "                train_files.append(os.path.join(folder, file))\n",
    "            # Test: suffix 19 to 22\n",
    "            if any(file.endswith(f\"{i:02d}.mat\") for i in test_range):\n",
    "                test_files.append(os.path.join(folder, file))\n",
    "    return train_files, test_files\n",
    "\n",
    "falls_folder = os.path.join(\"../data\", \"falls\")\n",
    "os.makedirs(falls_folder, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "train_range = range(1, 19)   # 01 to 18\n",
    "test_range = range(19, 23)   # 19 to 22\n",
    "\n",
    "train_files, test_files = split_falls_by_suffix(falls_folder, train_range, test_range)\n",
    "\n",
    "print(\"Train files:\", train_files)\n",
    "print(\"Test files:\", test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60e0af54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,)\n",
      "(0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "# Suppose you already have test files loaded from your folder\n",
    "# data_suffixes_group1 = [f\"{i:02d}\" for i in range(1, 19)]  # e.g., 19, 20\n",
    "# data_suffixes_group2 = [f\"{i:02d}\" for i in range(19, 28)]  # e.g., 21, 22\n",
    "test_suffixes_group1 = [f\"{i:02d}\" for i in range(19, 21)]  # 19, 20\n",
    "test_suffixes_group2 = [f\"{i:02d}\" for i in range(21, 23)]  # 21, 22\n",
    "\n",
    "def process_dataset_by_suffix(data_folder, suffixes):\n",
    "    features, labels = [], []\n",
    "    for label_folder, label in [('falls', 2), ('nonfalls', 1)]:\n",
    "        folder_path = os.path.join(data_folder, label_folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.mat') and any(file.endswith(f\"{s}.mat\") for s in suffixes):\n",
    "                path = os.path.join(folder_path, file)\n",
    "                features.append(extract_features_from_file(path))\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "DATA_FOLDER = \"../data\"\n",
    "\n",
    "X_test_group1, y_test_group1 = process_dataset_by_suffix(DATA_FOLDER, test_suffixes_group1)\n",
    "X_test_group2, y_test_group2 = process_dataset_by_suffix(DATA_FOLDER, test_suffixes_group2)\n",
    "\n",
    "print(X_test_group1.shape, y_train.shape)\n",
    "print(X_test_group2.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34907dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,)\n",
      "(0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "def process_dataset_by_suffix(data_folder, suffixes):\n",
    "    features, labels = [], []\n",
    "    for label_folder, label in [('falls', 2), ('nonfalls', 1)]:\n",
    "        folder_path = os.path.join(data_folder, label_folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.mat') and any(file.endswith(f\"{s}.mat\") for s in suffixes):\n",
    "                path = os.path.join(folder_path, file)\n",
    "                features.append(extract_features_from_file(path))\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Use suffixes for train and test\n",
    "train_suffixes = [f\"{i:02d}\" for i in range(1, 19)]\n",
    "test_suffixes = [f\"{i:02d}\" for i in range(19, 23)]\n",
    "\n",
    "DATA_FOLDER = \"../data\"  # Make sure this matches your folder structure\n",
    "\n",
    "X_train, y_train = process_dataset_by_suffix(DATA_FOLDER, train_suffixes)\n",
    "X_test, y_test = process_dataset_by_suffix(DATA_FOLDER, test_suffixes)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,)\n",
      "(0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(folders):\n",
    "    features, labels = [], []\n",
    "    for folder in folders:\n",
    "        for label_folder, label in [('falls', 2), ('nonfalls', 1)]:\n",
    "            folder_path = os.path.join(folder, label_folder)\n",
    "            if not os.path.exists(folder_path):\n",
    "                continue\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.endswith('.mat'):\n",
    "                    path = os.path.join(folder_path, file)\n",
    "                    features.append(extract_features_from_file(path))\n",
    "                    labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Define train and test folders\n",
    "train_folders = [os.path.join(DATA_FOLDER, f\"{i:02d}\") for i in range(1, 19)]\n",
    "test_folders = [os.path.join(DATA_FOLDER, f\"{i:02d}\") for i in range(19, 23)]\n",
    "\n",
    "X_train, y_train = process_dataset(train_folders)\n",
    "X_test, y_test = process_dataset(test_folders)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de45e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_file(filepath):\n",
    "    mat = scipy.io.loadmat(filepath)\n",
    "    data = mat['newData']\n",
    "    imu_data = data[:, 1:10]\n",
    "    means = np.mean(imu_data, axis=0)\n",
    "    max_vals = np.max(imu_data, axis=0)\n",
    "    std_devs = np.std(imu_data, axis=0)\n",
    "    return np.concatenate([means, max_vals, std_devs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35559477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(root_folder):\n",
    "    features, labels = [], []\n",
    "    for label_folder, label in [('falls', 2), ('nonfalls', 1)]:\n",
    "        folder_path = os.path.join(root_folder, label_folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.mat'):\n",
    "                path = os.path.join(folder_path, file)\n",
    "                features.append(extract_features_from_file(path))\n",
    "                labels.append(label)\n",
    "    return np.array(features), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44fa898",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m test_folders = [os.path.join(DATA_FOLDER, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m19\u001b[39m, \u001b[32m23\u001b[39m)]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Use the process_dataset function that accepts a list of folders (from cell 2)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_train, y_train = \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_folders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m X_test, y_test = process_dataset(test_folders)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mprocess_dataset\u001b[39m\u001b[34m(root_folder)\u001b[39m\n\u001b[32m      2\u001b[39m features, labels = [], []\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m label_folder, label \u001b[38;5;129;01min\u001b[39;00m [(\u001b[33m'\u001b[39m\u001b[33mfalls\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m2\u001b[39m), (\u001b[33m'\u001b[39m\u001b[33mnonfalls\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1\u001b[39m)]:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     folder_path = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os.listdir(folder_path):\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m'\u001b[39m\u001b[33m.mat\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen posixpath>:76\u001b[39m, in \u001b[36mjoin\u001b[39m\u001b[34m(a, *p)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# X_train, y_train = process_dataset(\"path_to_training_folder\")\n",
    "# X_test, y_test = process_dataset(\"path_to_testing_folder\")\n",
    "\n",
    "# Use the correct folder split for train and test\n",
    "# train_folders = [os.path.join(DATA_FOLDER, f\"{i:02d}\") for i in range(1, 19)]\n",
    "# test_folders = [os.path.join(DATA_FOLDER, f\"{i:02d}\") for i in range(19, 23)]\n",
    "\n",
    "# # Use the process_dataset function that accepts a list of folders (from cell 2)\n",
    "# X_train, y_train = process_dataset(train_folders)\n",
    "# X_test, y_test = process_dataset(test_folders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1739b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,)\n",
      "(0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe966c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m rf = \u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m gb = GradientBoostingClassifier().fit(X_train, y_train)\n\u001b[32m      3\u001b[39m svm = SVC(probability=\u001b[38;5;28;01mTrue\u001b[39;00m).fit(X_train, y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sklearn-env311/lib/python3.11/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sklearn-env311/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:359\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[32m    372\u001b[39m estimator = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator)(criterion=\u001b[38;5;28mself\u001b[39m.criterion)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sklearn-env311/lib/python3.11/site-packages/sklearn/utils/validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sklearn-env311/lib/python3.11/site-packages/sklearn/utils/validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sklearn-env311/lib/python3.11/site-packages/sklearn/utils/validation.py:1091\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1084\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1085\u001b[39m             msg = (\n\u001b[32m   1086\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1087\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1088\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1089\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mif it contains a single sample.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1090\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array.dtype, \u001b[33m\"\u001b[39m\u001b[33mkind\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUSV\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1094\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1095\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdtype=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1096\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1097\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected 2D array, got 1D array instead:\narray=[].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "gb = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "svm = SVC(probability=True).fit(X_train, y_train)\n",
    "xgb_model = xgb.XGBClassifier().fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_rf = shap.Explainer(rf, X_train)\n",
    "explainer_gb = shap.Explainer(gb, X_train)\n",
    "explainer_xgb = shap.Explainer(xgb_model, X_train)\n",
    "explainer_svm = shap.KernelExplainer(svm.predict_proba, X_train)\n",
    "\n",
    "shap_rf = explainer_rf(X_test)\n",
    "shap_gb = explainer_gb(X_test)\n",
    "shap_xgb = explainer_xgb(X_test)\n",
    "shap_svm = explainer_svm.shap_values(X_test)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd3db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f\"{stat}_{sensor}_{axis}\" for stat in ['mean','max','std']\n",
    "                 for sensor in ['acc','gyro','mag'] for axis in ['x','y','z']]\n",
    "\n",
    "def top_shap(shap_vals, model_name, feature_names):\n",
    "    mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "    return pd.DataFrame({'Feature': feature_names, model_name: mean_abs})\n",
    "\n",
    "df_rf = top_shap(shap_rf.values, 'Random Forest', feature_names)\n",
    "df_gb = top_shap(shap_gb.values, 'Gradient Boosting', feature_names)\n",
    "df_xgb = top_shap(shap_xgb.values, 'XGBoost', feature_names)\n",
    "df_svm = top_shap(shap_svm, 'SVM', feature_names)\n",
    "\n",
    "df_merge = df_rf.merge(df_gb, on='Feature', how='outer') \\\n",
    "                .merge(df_xgb, on='Feature', how='outer') \\\n",
    "                .merge(df_svm, on='Feature', how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534221c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt = df_merge.melt(id_vars='Feature', var_name='Model', value_name='SHAP Value')\n",
    "\n",
    "fig = px.bar(\n",
    "    df_melt, x='SHAP Value', y='Feature', color='Model',\n",
    "    orientation='h', barmode='group',\n",
    "    title='🎯 SHAP Feature Importance Battle Royale',\n",
    "    labels={'SHAP Value': 'Mean |SHAP| Value'}\n",
    ")\n",
    "fig.update_layout(template='plotly_dark')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323dbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "print(\"🔍 Random Forest Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
